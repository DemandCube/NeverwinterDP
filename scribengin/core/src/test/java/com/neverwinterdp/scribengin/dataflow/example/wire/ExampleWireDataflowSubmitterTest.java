package com.neverwinterdp.scribengin.dataflow.example.wire;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;

import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Properties;
import java.util.Random;

import kafka.consumer.Consumer;
import kafka.consumer.ConsumerConfig;
import kafka.consumer.ConsumerIterator;
import kafka.consumer.ConsumerTimeoutException;
import kafka.consumer.KafkaStream;
import kafka.javaapi.consumer.ConsumerConnector;
import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;

import org.apache.commons.lang.RandomStringUtils;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import com.neverwinterdp.scribengin.LocalScribenginCluster;
import com.neverwinterdp.scribengin.shell.ScribenginShell;

public class ExampleWireDataflowSubmitterTest {
  LocalScribenginCluster localScribenginCluster ;
  ScribenginShell shell;
  int numMessages = 10000;
  
  /**
   * Setup a local Scribengin cluster
   * This sets up kafka, zk, and vm-master
   * @throws Exception
   */
  @Before
  public void setup() throws Exception{
    String BASE_DIR = "build/working";
    System.setProperty("app.home", BASE_DIR + "/scribengin");
    System.setProperty("vm.app.dir", BASE_DIR + "/scribengin");
    
    localScribenginCluster = new LocalScribenginCluster(BASE_DIR) ;
    localScribenginCluster.clean(); 
    localScribenginCluster.useLog4jConfig("classpath:scribengin/log4j/vm-log4j.properties");  
    localScribenginCluster.start();
    
    shell = localScribenginCluster.getShell();
    
  }
  
  /**
   * Destroy the local Scribengin cluster and clean up 
   * @throws Exception
   */
  @After
  public void teardown() throws Exception{
    localScribenginCluster.shutdown();
  }
  
  /**
   * Test our Simple Dataflow Submitter
   * 1. Write data to Kafka into the input topic
   * 2. Run our dataflow
   * 3. Use a Kafka Consumer to read the data in the output topic and make sure its all present 
   * @throws Exception
   */
  @Test
  public void TestExampleWireDataflowSubmitterTest() throws Exception{
    //Create a new DataflowSubmitter with default properties
    ExampleWireDataflowSubmitter eds = new ExampleWireDataflowSubmitter(shell);
    
    //Populate kafka input topic with data
    sendKafkaData(localScribenginCluster.getKafkaCluster().getKafkaConnect(), eds.getInputTopic());
    
    //Submit the dataflow and wait for it to start running
    eds.submitDataflow(localScribenginCluster.getKafkaCluster().getZKConnect());
    //Output the registry for debugging purposes
    shell.execute("registry dump");
    
    //Get basic info on the dataflow
    shell.execute("dataflow info --dataflow-id "+eds.getDataflowID());
    
    //Get the kafka output topic iterator
    ConsumerIterator<byte[], byte[]> outputIterator = 
        getConsumerIterator(localScribenginCluster.getKafkaCluster().getZKConnect(), eds.getOutputTopic());
    
    //This topic is automatically generated by Scribengin as an intermediary topic
    //  between the splitter operator and the even operator
    ConsumerIterator<byte[], byte[]> evenIterator = 
        getConsumerIterator(localScribenginCluster.getKafkaCluster().getZKConnect(), "WireDataflow.splitteroperator-to-evenoperator");
    
    //This topic is automatically generated by Scribengin as an intermediary topic
    //  between the splitter operator and the odd operator
    ConsumerIterator<byte[], byte[]> oddIterator = 
        getConsumerIterator(localScribenginCluster.getKafkaCluster().getZKConnect(), "WireDataflow.splitteroperator-to-oddoperator");
    
    //Do some very simple verification to ensure our data has been moved correctly
    int numReceived =  getNumKafkaMessages(outputIterator);
    int evenCount = getNumKafkaMessages(evenIterator);
    int oddCount = getNumKafkaMessages(oddIterator);
    
    //The number of messages sent should match the number of messages in our final topic
    assertEquals(numMessages, numReceived);
    //The number of messages received should be the count of our two intermediate topics added together
    assertEquals(numReceived, evenCount+oddCount);
    //And finally, the intermediate topics should have more than zero messages
    assertTrue(evenCount > 0);
    assertTrue(oddCount > 0);
  }
  
  /**
   * Returns the number of messages in a kafka topic
   * @param i A Kafka ConsumerIterator
   * @return
   */
  private int getNumKafkaMessages(ConsumerIterator<byte[], byte[]> iterator){
    int result = 0;
    try{
      while(iterator.hasNext()){
        iterator.next();
        result++;
        //System.err.println(result);
      }
    } catch (ConsumerTimeoutException e){}
    
    return result;
  }
  
  /**
   * Sends Strings of random length to a Kafka topic
   * @param kafkaConnect [host]:[port] of Kafka
   * @param inputTopic The topic to write to
   */
  private void sendKafkaData(String kafkaConnect, String topic){
    Properties props = new Properties();
    props.put("metadata.broker.list", kafkaConnect);
    props.put("serializer.class", "kafka.serializer.StringEncoder");
    props.put("partitioner.class", "kafka.producer.DefaultPartitioner");
    props.put("request.required.acks", "1");
    ProducerConfig config = new ProducerConfig(props);
    Random rand = new Random();
    
    Producer<String, String> producer = new Producer<String, String>(config);
    for(int i = 0; i < numMessages; i++){
      String generatedString = RandomStringUtils.randomAlphabetic(rand.nextInt(10)+1);
      producer.send(new KeyedMessage<String, String>(topic, "test", generatedString));
    }
    producer.close();
  }
  
  /**
   * Returns an iterator to go through a given Kafka topic
   * @param zkConnect Zookeeper [host]:[port] kafka is connected ot
   * @param topic The topic to return an interator for
   * @return
   */
  private ConsumerIterator<byte[], byte[]> getConsumerIterator(String zkConnect, String topic){
    Properties props = new Properties();
    props.put("zookeeper.connect", zkConnect);
    props.put("group.id", "default");
    props.put("consumer.timeout.ms", "5000");
    
    ConsumerConfig consumerConfig = new ConsumerConfig(props);
    ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(consumerConfig);
    Map<String, Integer> topicCountMap = new HashMap<String, Integer>();
    topicCountMap.put(topic, new Integer(1));
    Map<String, List<KafkaStream<byte[], byte[]>>> consumerMap = consumerConnector.createMessageStreams(topicCountMap);
    KafkaStream<byte[], byte[]> stream =  consumerMap.get(topic).get(0);
    return stream.iterator();
  }
}
